====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
EncoderDecoderTransformer                          [8, 54, 10000]            --
├─SpeechEmbedding: 1-1                             [8, 464, 256]             --
│    └─Conv2DSubsampling: 2-1                      [8, 928, 256]             --
│    │    └─Sequential: 3-1                        [8, 256, 928, 76]         592,640
│    │    └─Linear: 3-2                            [8, 928, 256]             4,980,992
│    │    └─Dropout: 3-3                           [8, 928, 256]             --
│    └─StackedBLSTMEmbedding: 2-2                  [8, 464, 256]             --
│    │    └─LSTM: 3-4                              [8, 928, 256]             395,264
│    │    └─MaxPool1d: 3-5                         [8, 256, 464]             --
│    │    └─LSTM: 3-6                              [8, 464, 256]             395,264
│    │    └─MaxPool1d: 3-7                         [8, 256, 464]             --
│    │    └─Linear: 3-8                            [8, 464, 256]             65,792
│    │    └─Dropout: 3-9                           [8, 464, 256]             --
├─PositionalEncoding: 1-2                          [8, 464, 256]             --
├─Dropout: 1-3                                     [8, 464, 256]             --
├─ModuleList: 1-4                                  --                        --
│    └─SelfAttentionEncoderLayer: 2-3              [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-10               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-11                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-4              [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-12               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-13                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-5              [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-14               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-15                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-6              [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-16               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-17                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-7              [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-18               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-19                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-8              [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-20               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-21                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-9              [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-22               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-23                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-10             [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-24               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-25                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-11             [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-26               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-27                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-12             [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-28               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-29                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-13             [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-30               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-31                 [8, 464, 256]             526,080
│    └─SelfAttentionEncoderLayer: 2-14             [8, 464, 256]             --
│    │    └─SelfAttentionLayer: 3-32               [8, 464, 256]             263,680
│    │    └─FeedForwardLayer: 3-33                 [8, 464, 256]             526,080
├─LayerNorm: 1-5                                   [8, 464, 256]             512
├─Sequential: 1-6                                  [8, 464, 10000]           --
│    └─Linear: 2-15                                [8, 464, 10000]           2,570,000
│    └─LogSoftmax: 2-16                            [8, 464, 10000]           --
├─Embedding: 1-7                                   [8, 54, 256]              2,560,000
├─PositionalEncoding: 1-8                          [8, 54, 256]              --
├─Dropout: 1-9                                     [8, 54, 256]              --
├─ModuleList: 1-10                                 --                        --
│    └─CrossAttentionDecoderLayer: 2-17            [8, 54, 256]              --
│    │    └─SelfAttentionLayer: 3-34               [8, 54, 256]              263,680
│    │    └─CrossAttentionLayer: 3-35              [8, 54, 256]              263,680
│    │    └─FeedForwardLayer: 3-36                 [8, 54, 256]              526,080
│    └─CrossAttentionDecoderLayer: 2-18            [8, 54, 256]              --
│    │    └─SelfAttentionLayer: 3-37               [8, 54, 256]              263,680
│    │    └─CrossAttentionLayer: 3-38              [8, 54, 256]              263,680
│    │    └─FeedForwardLayer: 3-39                 [8, 54, 256]              526,080
│    └─CrossAttentionDecoderLayer: 2-19            [8, 54, 256]              --
│    │    └─SelfAttentionLayer: 3-40               [8, 54, 256]              263,680
│    │    └─CrossAttentionLayer: 3-41              [8, 54, 256]              263,680
│    │    └─FeedForwardLayer: 3-42                 [8, 54, 256]              526,080
│    └─CrossAttentionDecoderLayer: 2-20            [8, 54, 256]              --
│    │    └─SelfAttentionLayer: 3-43               [8, 54, 256]              263,680
│    │    └─CrossAttentionLayer: 3-44              [8, 54, 256]              263,680
│    │    └─FeedForwardLayer: 3-45                 [8, 54, 256]              526,080
│    └─CrossAttentionDecoderLayer: 2-21            [8, 54, 256]              --
│    │    └─SelfAttentionLayer: 3-46               [8, 54, 256]              263,680
│    │    └─CrossAttentionLayer: 3-47              [8, 54, 256]              263,680
│    │    └─FeedForwardLayer: 3-48                 [8, 54, 256]              526,080
│    └─CrossAttentionDecoderLayer: 2-22            [8, 54, 256]              --
│    │    └─SelfAttentionLayer: 3-49               [8, 54, 256]              263,680
│    │    └─CrossAttentionLayer: 3-50              [8, 54, 256]              263,680
│    │    └─FeedForwardLayer: 3-51                 [8, 54, 256]              526,080
├─LayerNorm: 1-11                                  [8, 54, 256]              512
├─Linear: 1-12                                     [8, 54, 10000]            2,570,000
====================================================================================================
Total params: 29,928,736
Trainable params: 29,928,736
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 339.00
====================================================================================================
Input size (MB): 4.77
Forward/backward pass size (MB): 3411.58
Params size (MB): 94.45
Estimated Total Size (MB): 3510.80
====================================================================================================